<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>强化学习 - J-shiro's Blog</title><meta name=Description content="Hugo theme - LoveIt"><meta property="og:url" content="https://j-shiro.github.io/reinforcement_learning_note/">
<meta property="og:site_name" content="J-shiro's Blog"><meta property="og:title" content="强化学习"><meta property="og:description" content="概念 要素：$state\overset{policy}\longrightarrow action$
奖励函数
Reward Function，衡量智能体某个状态 s 下采取动作 a 后获得的即时反馈"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-26T22:22:42+08:00"><meta property="og:image" content="https://j-shiro.github.io/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://j-shiro.github.io/logo.png"><meta name=twitter:title content="强化学习"><meta name=twitter:description content="概念 要素：$state\overset{policy}\longrightarrow action$
奖励函数
Reward Function，衡量智能体某个状态 s 下采取动作 a 后获得的即时反馈"><meta name=application-name content="LoveIt"><meta name=apple-mobile-web-app-title content="LoveIt"><meta name=referrer content="no-referrer"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://j-shiro.github.io/reinforcement_learning_note/><link rel=prev href=https://j-shiro.github.io/deep_learning_note/><link rel=next href=https://j-shiro.github.io/record_thesis/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"强化学习","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/j-shiro.github.io\/reinforcement_learning_note\/"},"image":["https:\/\/j-shiro.github.io\/images\/Apple-Devices-Preview.png"],"genre":"posts","wordcount":1648,"url":"https:\/\/j-shiro.github.io\/reinforcement_learning_note\/","datePublished":"2024-10-12T00:00:00+00:00","dateModified":"2025-05-26T22:22:42+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":{"@type":"ImageObject","url":"https:\/\/j-shiro.github.io\/images\/avatar.png","width":800,"height":800}},"author":{"@type":"Person","name":"jshiro"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="J-shiro's Blog"><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>所有文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/categories/documentation/>文档 </a><a class=menu-item href=/about/>关于 </a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/reinforcement_learning_note/ selected>简体中文</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="J-shiro's Blog"><span class=header-title-pre><i class='far fa-kiss-wink-heart fa-fw' aria-hidden=true></i></span>LoveIt</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>所有文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/categories/documentation/ title>文档</a><a class=menu-item href=/about/ title>关于</a><a class=menu-item href=https://github.com/dillonzq/LoveIt title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title=选择语言><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/reinforcement_learning_note/ selected>简体中文</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">强化学习</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>jshiro</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-12>2024-10-12</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 1648 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 4 分钟&nbsp;<span id=/reinforcement_learning_note/ class=leancloud_visitors data-flag-title=强化学习>
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#概念>概念</a></li><li><a href=#模型>模型</a><ul><li><a href=#mdp>MDP</a></li><li><a href=#mcts>MCTS</a></li></ul></li><li><a href=#深度强化学习>深度强化学习</a></li></ul></nav></div></div><div class=content id=content><h2 id=概念>概念</h2><p>要素：$state\overset{policy}\longrightarrow action$</p><p><strong>奖励函数</strong></p><ul><li><p>Reward Function，衡量智能体某个状态 s 下采取动作 a 后获得的即时反馈</p></li><li><p>$R(s,a)$，通过奖励好行为和惩罚坏行为使自动学习，</p></li></ul><blockquote><p>$(s_{start}, a, r(s_{start}, s_{change}))$</p><p>在初始状态$s_{start}$采取动作$a$，得到奖励$r(s_{start})$，状态转移为$s_{change}$</p></blockquote><p><strong>回报</strong></p><ul><li><p>return，从当前时刻到未来所有奖励总和，<strong>加权和</strong>，每一步奖励都乘以折扣因子</p></li><li><p>折扣因子(discount factor)：$\gamma=0.9$，奖励随动作增加减少</p></li></ul><p>$return=R_1 \cdot (\gamma) + R_2 \cdot (\gamma)^2 + \cdots + R_n\cdot (\gamma)^n$</p><p><strong>策略函数</strong></p><ul><li><p>policy，策略 $\pi$ 为智能体在每个状态 $s$ 下选择动作 $a$ 的行为准则</p><ul><li>确定性：$\pi(s)=a$，状态 $s$ 下一定选择动作 $a$</li><li>随机性：$\pi(a|s)=P(A=a|S=s)$，状态 $s$ 下选择动作 $a$ 的概率</li></ul></li><li><p>目标：最大化回报</p></li></ul><p><strong>连续状态空间</strong></p><p>每个状态不是离散的，而是通过向量表示：$s=[x,y,\theta,x&rsquo;,y&rsquo;,\theta&rsquo;]$可分别表示位置，方向，速度，角度等</p><p>该状态空间下，无法用表格表示所有状态，而是需要使用函数近似器：神经网络</p><p><strong>ε-贪婪策略</strong></p><p>强化学习中，智能体面临两难问题</p><p>1️⃣ > 利用exploitation：利用已有知识选择当前最好动作，最大化Q(s, a)</p><p>2️⃣ > 探索exploration：尝试其他动作，可能发现更好策略</p><ul><li><p>ε = 0.05，逐渐减小 ε 使得前期多探索，后期稳定利用最优策略</p></li><li><p><strong>0.95概率选择最大化 Q(s,a) 的动作 a：exploitation</strong></p></li><li><p><strong>0.05概率选择随机的动作 a ：exploration</strong></p></li></ul><p><strong>软更新</strong></p><ul><li>背景：深度 Q 网络（DQN）不稳定，若主网络每次更新都直接同步到目标网络，会导致训练不稳定<ul><li>主网络（online network）：$Q_{\theta}$，实时更新，选择动作</li><li>目标网络（target network）：$Q_{\theta^{&rsquo;}}$，生成目标值，更新慢，保持稳定</li></ul></li></ul><p>当使用小批量梯度下降时，更新 Q 的参数 w 及 b 时软更新，利于收敛：</p><ul><li><p>$w=0.01w_{new}+0.99w$</p></li><li><p>$b=0.01b_{new}+0.99b$</p></li><li><p>$w_{new}$为主网络参数，$w$为目标网络参数，$\tau=0.01$为软更新系数</p></li></ul><h2 id=模型>模型</h2><h3 id=mdp>MDP</h3><p><strong>马尔科夫决策过程</strong>：Markov Decision Process (MDP)</p><blockquote><p>四元组：(S, A, P, R)</p><ul><li>S：状态空间</li><li>A：动作空间</li><li>P(s&rsquo;|s, a)：状态转移概率，从 s 采取动作 a 到状态 s&rsquo; 的概率</li><li>R(s, a)：奖励函数，从 s 采取 动作 a 得到的即时奖励</li></ul></blockquote><p>性质未来只取决于现在状态，与过去无关</p><p><strong>状态-动作价值函数（Q-Function, Q(s, a)）</strong></p><p>Q函数表示在状态 s 下采取动作 a 后<strong>期望总回报</strong></p><ul><li>最优Q函数：Q*(s, a)，从 s 开始，执行动作 a，按照最优策略执行后续动作，最终获得的最大期望回报</li><li>构建最优策略：$\pi^{*}(s)=\arg \max\limits_{a}Q^{*}(s,a)$</li></ul><p><strong>贝尔曼方程(Bellman Equation)</strong></p><blockquote><p>当前 Q 值 = 当前奖励 + 未来最大奖励的折扣期望</p><p>提供训练目标</p></blockquote><p>1️⃣ 确定性环境：每个动作对应唯一下一个状态</p><ul><li>$Q(s,a)=R(s, a)+\gamma \max\limits_{a&rsquo;}Q(s&rsquo;,a&rsquo;)=R_1+\gamma R_2 +\gamma^2 R_3 +\cdots= R_1+\gamma[R_2+\gamma R_3 +\cdots]$</li></ul><p>2️⃣ 随机性环境：下一状态和奖励是概率分布</p><ul><li><p>当出现随机情况，即下一个状态可能未实现，则更关注的是最大化多次运动下的<strong>期望回报</strong></p></li><li><p>$Expected,,Return = Average(R_1+\gamma R_2+\gamma^2 R_3 +\cdots)=E[R_1+\gamma R_2+\gamma^2 R_3 +\cdots]$</p></li><li><p>$Q(s,a)=R(s)+\gamma E_{s^{&rsquo;}}[\max\limits_{a&rsquo;}Q(s&rsquo;,a&rsquo;)]$</p></li></ul><h3 id=mcts>MCTS</h3><p>蒙特卡洛树搜索，The Monte Carlo Tree Search，给定一个游戏状态，选择最佳下一步</p><ul><li><p>选择selection：选择最大化UCB值的结点，$UCB(S_i)=\overline{V_i}+c\sqrt{\frac{\log N}{n_i}}, c=2$</p><ul><li>$V_i$ 指该结点下平均价值，N探索次数，n当前结点探索次数</li></ul></li><li><p>扩展node_expansion：创建一个或多个子节点</p></li><li><p>仿真Rollout：某一节点用随机策略进行游戏</p></li><li><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title=复制到剪贴板><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Def</span> <span class=n>Rollout</span><span class=p>(</span><span class=n>S_i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>loop</span> <span class=n>forever</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>S_i</span> <span class=ow>is</span> <span class=n>a</span> <span class=n>terminal</span> <span class=n>state</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=k>return</span> <span class=n>value</span><span class=p>(</span><span class=n>S_i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>A_i</span> <span class=o>=</span> <span class=n>random</span><span class=p>(</span><span class=n>available</span><span class=o>-</span><span class=n>actions</span><span class=p>(</span><span class=n>S_i</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=n>S_i</span> <span class=o>=</span> <span class=n>simulate</span><span class=p>(</span><span class=n>A_i</span><span class=p>,</span> <span class=n>S_i</span><span class=p>)</span></span></span></code></pre></div></div></li><li><p>反向传播back propagation：使用随机搜索结果更新搜索树，价值累加，探索次数加一</p></li></ul><img src=/img/reinforcement_learning_note.zh-cn.assets/image-20241009232005644.png alt=图片无法加载><ol><li>通过计算UCB最大值一直到叶结点，查看是否探索过，未探索过则仿真</li><li>探索过则枚举当前结点所有可能动作添加到树，扩展一个新结点</li></ol><h2 id=深度强化学习>深度强化学习</h2><ul><li>Deep Reinforcement Learning, DRL</li></ul><p><strong>学习状态值函数</strong></p><ul><li>使用神经网络去学习近似Q函数或策略函数，选择最大化 Q(s,a) 的动作 a</li><li>训练目标：<strong>最小化贝尔曼误差</strong></li></ul><p>$$Loss(\theta)=(Q_{\theta}(s,a) - [r + \gamma \max\limits_{a^{&rsquo;}}Q_{\theta}-(s^{&rsquo;},a^{&rsquo;})])^2$$</p><img src=/img/reinforcement_learning_note.zh-cn.assets/172845431716542.png alt=图片加载失败></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2025-05-26&nbsp;<a class=git-hash href=https://github.com/dillonzq/LoveIt/commit/6ac9dcd291791f3c3cb749571f629dc97d88b9c8 target=_blank title="commit by J-shiro(541737231@qq.com) 6ac9dcd291791f3c3cb749571f629dc97d88b9c8: fix the math error in reinforcement learning note">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>6ac9dcd</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/reinforcement_learning_note/index.md target=_blank>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://j-shiro.github.io/reinforcement_learning_note/><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://j-shiro.github.io/reinforcement_learning_note/ data-title=强化学习><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@14.9.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://j-shiro.github.io/reinforcement_learning_note/ data-title=强化学习><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a><a href="https://t.me/share/url?url=https%3a%2f%2fj-shiro.github.io%2freinforcement_learning_note%2f&amp;text=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0" target=_blank title="分享到 Telegram"><i class="fab fa-telegram fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/deep_learning_note/ class=prev rel=prev title=深度学习><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>深度学习</a>
<a href=/record_thesis/ class=next rel=next title=论文阅读记录>论文阅读记录<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.143.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.1-DEV"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2019 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>jshiro</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css><script src=https://cdn.jsdelivr.net/npm/valine@1.5.3/dist/Valine.min.js></script><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@5.20.2/dist/lite/builds/browser.umd.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/copy-tex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/mhchem.min.js></script><script>window.config={comment:{valine:{appId:"wAgpK3Sa13Vfm247ncxiGVDA-MdYXbMMI",appKey:"p6X9gNaJTGB1sqgG9ZnrEwy4",avatar:"mp",el:"#valine",emojiCDN:"https://cdn.jsdelivr.net/npm/emoji-datasource-google@15.1.2/img/google/64/",emojiMaps:{100:"1f4af.png",alien:"1f47d.png",anger:"1f4a2.png",angry:"1f620.png",anguished:"1f627.png",astonished:"1f632.png",black_heart:"1f5a4.png",blue_heart:"1f499.png",blush:"1f60a.png",bomb:"1f4a3.png",boom:"1f4a5.png",broken_heart:"1f494.png",brown_heart:"1f90e.png",clown_face:"1f921.png",cold_face:"1f976.png",cold_sweat:"1f630.png",confounded:"1f616.png",confused:"1f615.png",cry:"1f622.png",crying_cat_face:"1f63f.png",cupid:"1f498.png",dash:"1f4a8.png",disappointed:"1f61e.png",disappointed_relieved:"1f625.png",dizzy:"1f4ab.png",dizzy_face:"1f635.png",drooling_face:"1f924.png",exploding_head:"1f92f.png",expressionless:"1f611.png",face_vomiting:"1f92e.png",face_with_cowboy_hat:"1f920.png",face_with_hand_over_mouth:"1f92d.png",face_with_head_bandage:"1f915.png",face_with_monocle:"1f9d0.png",face_with_raised_eyebrow:"1f928.png",face_with_rolling_eyes:"1f644.png",face_with_symbols_on_mouth:"1f92c.png",face_with_thermometer:"1f912.png",fearful:"1f628.png",flushed:"1f633.png",frowning:"1f626.png",ghost:"1f47b.png",gift_heart:"1f49d.png",green_heart:"1f49a.png",grimacing:"1f62c.png",grin:"1f601.png",grinning:"1f600.png",hankey:"1f4a9.png",hear_no_evil:"1f649.png",heart:"2764-fe0f.png",heart_decoration:"1f49f.png",heart_eyes:"1f60d.png",heart_eyes_cat:"1f63b.png",heartbeat:"1f493.png",heartpulse:"1f497.png",heavy_heart_exclamation_mark_ornament:"2763-fe0f.png",hole:"1f573-fe0f.png",hot_face:"1f975.png",hugging_face:"1f917.png",hushed:"1f62f.png",imp:"1f47f.png",innocent:"1f607.png",japanese_goblin:"1f47a.png",japanese_ogre:"1f479.png",joy:"1f602.png",joy_cat:"1f639.png",kiss:"1f48b.png",kissing:"1f617.png",kissing_cat:"1f63d.png",kissing_closed_eyes:"1f61a.png",kissing_heart:"1f618.png",kissing_smiling_eyes:"1f619.png",laughing:"1f606.png",left_speech_bubble:"1f5e8-fe0f.png",love_letter:"1f48c.png",lying_face:"1f925.png",mask:"1f637.png",money_mouth_face:"1f911.png",nauseated_face:"1f922.png",nerd_face:"1f913.png",neutral_face:"1f610.png",no_mouth:"1f636.png",open_mouth:"1f62e.png",orange_heart:"1f9e1.png",partying_face:"1f973.png",pensive:"1f614.png",persevere:"1f623.png",pleading_face:"1f97a.png",pouting_cat:"1f63e.png",purple_heart:"1f49c.png",rage:"1f621.png",relaxed:"263a-fe0f.png",relieved:"1f60c.png",revolving_hearts:"1f49e.png",right_anger_bubble:"1f5ef-fe0f.png",robot_face:"1f916.png",rolling_on_the_floor_laughing:"1f923.png",scream:"1f631.png",scream_cat:"1f640.png",see_no_evil:"1f648.png",shushing_face:"1f92b.png",skull:"1f480.png",skull_and_crossbones:"2620-fe0f.png",sleeping:"1f634.png",sleepy:"1f62a.png",slightly_frowning_face:"1f641.png",slightly_smiling_face:"1f642.png",smile:"1f604.png",smile_cat:"1f638.png",smiley:"1f603.png",smiley_cat:"1f63a.png",smiling_face_with_3_hearts:"1f970.png",smiling_imp:"1f608.png",smirk:"1f60f.png",smirk_cat:"1f63c.png",sneezing_face:"1f927.png",sob:"1f62d.png",space_invader:"1f47e.png",sparkling_heart:"1f496.png",speak_no_evil:"1f64a.png",speech_balloon:"1f4ac.png","star-struck":"1f929.png",stuck_out_tongue:"1f61b.png",stuck_out_tongue_closed_eyes:"1f61d.png",stuck_out_tongue_winking_eye:"1f61c.png",sunglasses:"1f60e.png",sweat:"1f613.png",sweat_drops:"1f4a6.png",sweat_smile:"1f605.png",thinking_face:"1f914.png",thought_balloon:"1f4ad.png",tired_face:"1f62b.png",triumph:"1f624.png",two_hearts:"1f495.png",unamused:"1f612.png",upside_down_face:"1f643.png",weary:"1f629.png",white_frowning_face:"2639-fe0f.png",white_heart:"1f90d.png",wink:"1f609.png",woozy_face:"1f974.png",worried:"1f61f.png",yawning_face:"1f971.png",yellow_heart:"1f49b.png",yum:"1f60b.png",zany_face:"1f92a.png",zipper_mouth_face:"1f910.png",zzz:"1f4a4.png"},enableQQ:!1,highlight:!0,lang:"zh-CN",pageSize:10,placeholder:"你的评论 ...",recordIP:!0,serverURLs:"https://wagpk3sa.api.lncldglobal.com",visitor:!0}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"GZT24PWKNT",algoliaIndex:"index",algoliaSearchKey:"634ab679e825b815de2663de38908f9d",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"}}</script><script src=/js/theme.min.js></script></body></html>